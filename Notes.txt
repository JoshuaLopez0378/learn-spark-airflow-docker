References:
  Pyspark 
  - https://medium.com/@SaphE/testing-apache-spark-locally-docker-compose-and-kubernetes-deployment-94d35a54f222

  Airflow + Pyspark
  - https://www.youtube.com/watch?v=o_pne3aLW2w
    - repo used - https://github.com/airscholar/SparkingFlow


Followed some instructions for each
Docker for pyspark: In root dir
- docker build -t our-own-apache-spark:3.4.0 .
- docker-compose up -d

Docker for airflow: In airflow dir
- can just run with docker-compose up -d, cd first to that dir. use "--build" force rebuild
- note by video: webserver usually not starts after first docker compose up. just do another docker-compose up -d (no --rebuild)

Set Connection in airflow 
- Error if not set: org.apache.spark.SparkException: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.
- Set:
  - Connection ID = spark-conn
  - Connection Type = Spark 
  - Host = spark://spark-master-1
  - Port = 7077
  - Deploy mode = client 
  - Spark binar = spark-submit

==========
Reminder:
- Force rebuild
  - docker-compose up -d --build

- Log as root in container
  - docker exec -u root -i -t <<container id>> /bin/bash

